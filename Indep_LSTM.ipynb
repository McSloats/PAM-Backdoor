{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Project5_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/McSloats/PAM-Backdoor/blob/master/Indep_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZlymTSwrxPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from sklearn import preprocessing\n",
        "from keras import optimizers as op\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import model_from_json\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAwClx-6Dlqb",
        "colab_type": "code",
        "outputId": "24f7a216-2744-492c-b246-0b90ed8590c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "##IMPORT FILE FOR DATA EXTRACTION\n",
        "benign = pd.read_csv(\"/content/drive/Shared drives/Machine Learning Project 5/benign.csv\")\n",
        "malicious = pd.read_csv(\"/content/drive/Shared drives/Machine Learning Project 5/malicious.csv\")\n",
        "#CREATE SINGLE DATAFRAME FOR BOTH THE BENIGN AND MALICIOUS DATA WITH MIXED ROWS\n",
        "benign = benign.sample(frac=1,random_state=1)\n",
        "malicious = malicious.sample(frac=1,random_state=1)\n",
        "data = [benign,malicious]\n",
        "data = pd.concat(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2,4,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KppaqOJxm8Ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##INSERT SECTION FOR DATA PRE-PROCESSING \n",
        "x = data\n",
        "labels = list(x.columns.values)\n",
        "data = None\n",
        "benign = None \n",
        "malicious = None\n",
        "x = x.fillna(0)\n",
        "X = x.replace(np.inf, 0)\n",
        "x = None\n",
        "\n",
        "#IDENTIFY INDEXES OF LABELLED ENTRIES AND REMOVE THEM\n",
        "temp = X.to_numpy()\n",
        "index = []\n",
        "count  = 0 \n",
        "newArray = []\n",
        "for i in temp:\n",
        "  if \"Src Port\" in i:\n",
        "    index.append(count)\n",
        "  count += 1\n",
        "\n",
        "val = 0 \n",
        "for i in temp:\n",
        "  if val not in index:\n",
        "    newArray.append(i)\n",
        "  val += 1\n",
        "\n",
        "val = None\n",
        "X = None\n",
        "\n",
        "#SLICE 'N DICE\n",
        "#CHECK TO MAKE SURE LABELLED ENTRIES ARE REMOVED\n",
        "count = 0 \n",
        "for i in newArray:\n",
        "  if \"Src Port\" in i:\n",
        "    print(count)\n",
        "  count += 1\n",
        "\n",
        "#REMOVE INFINITY VALUES \n",
        "array = []\n",
        "for i in newArray:\n",
        "  if 'Infinity' in i:\n",
        "    temp = []\n",
        "    for j in i:\n",
        "      if j == 'Infinity':\n",
        "        temp.append(0)\n",
        "      else:\n",
        "        temp.append(j)\n",
        "    array.append(temp)\n",
        "  else:\n",
        "    array.append(i)\n",
        "newArray = array\n",
        "\n",
        "#Remove all zeroed entries or entries with no Flow ID \n",
        "new = []\n",
        "for i in newArray:\n",
        "  if i[0] != 0:\n",
        "    new.append(i)\n",
        "newArray = new\n",
        "temp = None\n",
        "\n",
        "df = pd.DataFrame(newArray,columns=labels)#Create new DF for data before removal and normalisation \n",
        "\n",
        "##DROP ATTRIBUTES FOR FEATURE SELECTION HERE \n",
        "df1 = df.drop(columns=labels[6]) # Timestamp\n",
        "labels = list(df1.columns.values)\n",
        "#print(list(df1.columns.values))\n",
        "df1 = df1.drop(columns=[labels[1],labels[3]]) # IP Addresses. --> TO CATEGORICAL \n",
        "labels = list(df1.columns.values)\n",
        "#print(\"Remaining Lables:\\n\",labels)\n",
        "\n",
        "Y = df1[labels[-1]]\n",
        "ids = df1[labels[0]]\n",
        "cats = ids\n",
        "X1 = df1.drop(columns=labels[0]) # Flow ID --> USE FOR PUTTING DATA TOGETHER FOR TIME SERIES\n",
        "X1 = X1.drop(columns=labels[-1])\n",
        "df = None\n",
        "df1 = None\n",
        "\n",
        "newY = []\n",
        "for i in Y:\n",
        "  if i == 'ben':\n",
        "    newY.append(0)\n",
        "  else:\n",
        "    newY.append(1)\n",
        "\n",
        "labelsNew = list(X1.columns.values)\n",
        "for col in labelsNew:\n",
        "  X1[col] = pd.to_numeric(X1[col])\n",
        "\n",
        "#NORMALISE DATA\n",
        "# Create a minimum and maximum processor object\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "# Create an object to transform the data to fit minmax processor\n",
        "x_scaled = min_max_scaler.fit_transform(X1)\n",
        "\n",
        "# Run the normalizer on the dataframe\n",
        "df_normalized = pd.DataFrame(x_scaled,columns=labelsNew)\n",
        "x_scaled = None\n",
        "\n",
        "ids, _ = pd.factorize(cats)\n",
        "\n",
        "ids = pd.DataFrame(ids,columns=['Flow_ID'])\n",
        "\n",
        "# Create a minimum and maximum processor object\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "# Create an object to transform the data to fit minmax processor\n",
        "scaled = min_max_scaler.fit_transform(ids)\n",
        "\n",
        "ids = pd.DataFrame(scaled)\n",
        "\n",
        "Y = pd.DataFrame(newY,columns=['Label'])\n",
        "data = pd.concat([ids,df_normalized,Y],axis=1,sort=False) \n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "X = data.iloc[:,:-1].values\n",
        "Y = data.iloc[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzCeSiGMAOLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) ##ADJUST 'test_size' ACCORDINGLY \n",
        "y_test = to_categorical(y_test)\n",
        "y_train = to_categorical(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7lvHir3r9aE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn(x_train, y_train, x_test, y_test):\n",
        "    #KERAS MODEL    \n",
        "    #create model\n",
        "    model = Sequential()\n",
        "    #get number of columns in training data\n",
        "    n_cols = x_train.shape[1]\n",
        "    #add model layers\n",
        "    model.add(Embedding(100000, 80,input_length=n_cols))\n",
        "    model.add(LSTM(192,return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(96,return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(48))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "   \n",
        "    #Optimizer\n",
        "    opt = op.Adam()\n",
        "    #Compile model\n",
        "    model.compile(opt, loss='binary_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "   \n",
        "    #TRAINING\n",
        "    #Put this var in callbacks to save model after each epoch\n",
        "    #checkpointer = ModelCheckpoint('model/model-{epoch:02d}.hdf5', verbose=1)\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=256, \n",
        "              validation_split=0.2, epochs=10)\n",
        "\n",
        "    #test results\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqmjxrEGLhom",
        "colab_type": "code",
        "outputId": "c49c3f53-b843-4352-e8d1-636deddcd743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUXmAu9cEFay",
        "colab_type": "code",
        "outputId": "6d47bc6a-99f8-45ef-9bf2-f5533ed0c018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "rnn(x_train, y_train, x_test, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 543915 samples, validate on 135979 samples\n",
            "Epoch 1/10\n",
            " 30600/543915 [>.............................] - ETA: 38:40 - loss: 0.5691 - acc: 0.7144"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ac596ad195dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-b717546298e1>\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     model.fit(x_train, y_train, batch_size=200, \n\u001b[0;32m---> 28\u001b[0;31m               validation_split=0.2, epochs=10)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}